{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d6954d0-2fdf-425c-9e7e-c542410f0c27",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- IMPORTS (REFATORED) ---\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import os\n",
    "# Adiciona o diret√≥rio atual ao path para encontrar o m√≥dulo 'src'\n",
    "sys.path.append(os.getcwd())\n",
    "\n",
    "from src.ingestion.connectors import connect_jdbc, url_jdbc\n",
    "from src.ingestion.feature_store import salvar_feature_table\n",
    "\n",
    "from pyspark.sql.functions import col, to_date, lit\n",
    "from databricks.feature_engineering import FeatureEngineeringClient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e84423dc-2452-44a2-8dbb-77befca06694",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- CONFIGURA√á√ïES GLOBAIS DE OTIMIZA√á√ÉO (BEST PRACTICES) ---\n",
    "# Ativa otimiza√ß√£o autom√°tica de grava√ß√µes e compacta√ß√£o\n",
    "spark.conf.set(\"spark.databricks.delta.optimizeWrite.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.databricks.delta.autoCompact.enabled\", \"true\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35e0eff4-82af-4cae-909b-d2e6614af523",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "list_tables = [\n",
    "'CVC.BIP_vHISTORICO_SUPORTE_CANAL_LOJA_DATABRICKS'\n",
    ",'CVC.BIP_vHISTORICO_TARGUET_LOJA_DATABRICKS'\n",
    ",'CVC.BIP_vHISTORICO_FERIADOS_LOJA_DATABRICKS'\n",
    ",'CVC.BIP_vHISTORICO_SUPORTE_CANAL_ECOMMERCE_DATABRICKS'\n",
    ",'CVC.BIP_vHISTORICO_TARGUET_ECOMMERCE_DATABRICKS'\n",
    ",'CVC.BIP_vHISTORICO_SUPORTE_CANAL_VENDA_DIRETA_DATABRICKS'\n",
    ",'CVC.BIP_vHISTORICO_TARGUET_VENDA_DIRETA_DATABRICKS'\n",
    ",'CVC.CMC_aLOJAS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e9b84b6-fce8-4815-bf63-58bf9f3e9d82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- 1. CONFIGURA√á√ÉO DE CREDENCIAIS E CONEX√ÉO ---\n",
    "# Tenta pegar do Key Vault (Databricks Secrets) ou usa valores manuais (placeholder)\n",
    "try:\n",
    "    tenant_id = dbutils.secrets.get(scope=\"ss-keyvault-dev\", key=\"FBC-API-TENANT-ID\")\n",
    "    client_id = dbutils.secrets.get(scope=\"ss-keyvault-dev\", key=\"FBC-API-CLIENT-ID\")\n",
    "    client_secret = dbutils.secrets.get(scope=\"ss-keyvault-dev\", key=\"FBC-API-CLIENT-SECRET\")\n",
    "    print(\"‚úÖ Credenciais recuperadas do Key Vault com sucesso.\")\n",
    "except:\n",
    "    print(\"‚ö†Ô∏è Segredos n√£o encontrados. Usando valores manuais.\")\n",
    "\n",
    "scope = \"https://database.windows.net/.default\"\n",
    "sql_endpoint = \"sql-mi-cvc-prod.database.windows.net\" # Ajuste conforme necess√°rio\n",
    "database_name = \"DSP\"\n",
    "\n",
    "# Gera propriedades de conex√£o\n",
    "connection_properties = connect_jdbc(tenant_id, client_id, client_secret, scope)\n",
    "jdbc_url_str = url_jdbc(sql_endpoint, database_name)\n",
    "\n",
    "# --- 2. MAPEAMENTO DE CHAVES (PKs) E TIMESTAMP ---\n",
    "# Define PKs e Timestamp para cada tabela para garantir unicidade no Feature Store\n",
    "table_config = {\n",
    "    'CVC.BIP_vHISTORICO_SUPORTE_CANAL_LOJA_DATABRICKS': {'pks': [\"METRICAS\"], 'ts': \"DATA\"},\n",
    "    'CVC.BIP_vHISTORICO_TARGUET_LOJA_DATABRICKS': {'pks': [\"CODIGO_LOJA\"], 'ts': \"DATA\"},\n",
    "    'CVC.BIP_vHISTORICO_FERIADOS_LOJA_DATABRICKS': {'pks': [\"CODIGO_LOJA\"], 'ts': \"DATA\"},\n",
    "    'CVC.BIP_vHISTORICO_SUPORTE_CANAL_ECOMMERCE_DATABRICKS': {'pks': [\"METRICAS\"], 'ts': \"DATA\"},\n",
    "    'CVC.BIP_vHISTORICO_TARGUET_ECOMMERCE_DATABRICKS': {'pks': [\"CODIGO_LOJA\"], 'ts': \"DATA\"},\n",
    "    'CVC.BIP_vHISTORICO_SUPORTE_CANAL_VENDA_DIRETA_DATABRICKS': {'pks': [\"METRICAS\"], 'ts': \"DATA\"},\n",
    "    'CVC.BIP_vHISTORICO_TARGUET_VENDA_DIRETA_DATABRICKS': {'pks': [\"CODIGO_LOJA\"], 'ts': \"DATA\"},\n",
    "    'CVC.CMC_aLOJAS': {'pks': [\"CODIGO_LOJA\"], 'ts': None}\n",
    "}\n",
    "# --- 3. LOOP DE INGEST√ÉO ---\n",
    "print(f\"üöÄ Iniciando ingest√£o de {len(list_tables)} tabelas...\")\n",
    "\n",
    "for table_source in list_tables:\n",
    "    table_name_clean = table_source.split('.')[-1] # Ex: BIP_vHISTORICO...\n",
    "    target_table_name = f\"cvc.features.{table_name_clean.lower()}\" # Ex: cvc.features.bip_vhistorico...\n",
    "    \n",
    "    # Verifica configura√ß√£o\n",
    "    if table_source in table_config:\n",
    "        config = table_config[table_source]\n",
    "        pks = config['pks']\n",
    "        timestamp_col = config['ts']\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Configura√ß√£o n√£o encontrada para {table_source}. Pulando...\")\n",
    "        continue\n",
    "\n",
    "    print(f\"\\nüì¶ Processando: {table_source} -> {target_table_name}\")\n",
    "    \n",
    "    try:\n",
    "        # Leitura JDBC\n",
    "        print(f\"   üì• Lendo do SQL Server...\")\n",
    "        df_source = spark.read.jdbc(\n",
    "            url=jdbc_url_str,\n",
    "            table=table_source,\n",
    "            properties=connection_properties\n",
    "        )\n",
    "        \n",
    "        # Opcional: Casting de colunas de data se necess√°rio (Exemplo gen√©rico)\n",
    "        # if timestamp_col:\n",
    "        #     df_source = df_source.withColumn(timestamp_col, to_date(col(timestamp_col)))\n",
    "\n",
    "        # Grava√ß√£o no Feature Store\n",
    "        print(f\"   üíæ Salvando no Feature Store...\")\n",
    "        salvar_feature_table(\n",
    "            df=df_source,\n",
    "            table_name_full=target_table_name,\n",
    "            pk_columns=pks,\n",
    "            timestamp_col=timestamp_col,\n",
    "            spark=spark\n",
    "        )\n",
    "        print(f\"‚úÖ Sucesso: {target_table_name}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erro ao processar {table_source}: {str(e)}\")\n",
    "\n",
    "# Remove vari√°veis sens√≠veis da mem√≥ria\n",
    "del connection_properties\n",
    "del client_secret\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "cvc_ingestao_features_validacao",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
