"""
M√≥dulo de Prepara√ß√£o de Dados (Validation).

Este m√≥dulo √© respons√°vel por buscar os dados brutos no Feature Store e transform√°-los
no formato exigido pela biblioteca Darts (TimeSeries). Ele lida com a complexidade de:
1. Unir tabelas de features (Join Point-in-time).
2. Materializar dados do Spark para Pandas (necess√°rio para o Darts).
3. Criar objetos TimeSeries para Targets (Vendas) e Covari√°veis (Feriados, Indicadores).
4. Alinhar temporalmente s√©ries globais e locais.

Classes:
- DataIngestion: Orquestra todo o fluxo de dados.
"""

from typing import List, Tuple, Optional, Any
from databricks.feature_engineering import FeatureEngineeringClient, FeatureLookup
import pyspark.sql.functions as F
from pyspark.sql import SparkSession, DataFrame
import pandas as pd
import numpy as np
from darts import TimeSeries
from darts.utils.timeseries_generation import datetime_attribute_timeseries

class DataIngestion:
    """
    Controlador de Ingest√£o e Transforma√ß√£o de Dados.
    """
    def __init__(self, spark_session: SparkSession, config: Any):
        self.spark: SparkSession = spark_session
        self.config = config
        self.fe = FeatureEngineeringClient()

    def create_training_set(self) -> DataFrame:
        """
        Constr√≥i o Dataset de Treinamento unindo a tabela alvo com suas features.
        
        Utiliza o `FeatureEngineeringClient` para realizar um "Point-in-Time Join" correto,
        garantindo que para cada venda em data T, apenas as features conhecidas em T (ou antes)
        sejam associadas, evitando vazamento de dados do futuro (Data Leakage).
        
        Returns:
            DataFrame: DataFrame Spark contendo todas as colunas de dados e features unificadas.
        """
        print("üõí Construindo Training Set via Feature Store (Spark Native)...")
        target_table = f"{self.config.CATALOG}.{self.config.SCHEMA}.historico_targuet_loja"
        
        # Define a "Espinha Dorsal" (Spine) do dataset: Quem (Loja) e Quando (Data) queremos prever.
        df_spine = (self.spark.table(target_table)
                    .filter(F.col("data").between(self.config.DATA_START, self.config.INGESTION_END))
                    .select("codigo_loja", "data", "valor")
                    .withColumnRenamed("valor", "target_vendas")
                    .withColumn("codigo_loja", F.col("codigo_loja").cast("string"))
                   )

        # Configura os lookups para buscar features adicionais baseadas na chave (codigo_loja)
        # Feature 1: Caracter√≠sticas est√°ticas da loja (Cluster, UF, Tipo)
        # Feature 2: Dados hist√≥ricos de feriados (Time-series)
        feature_lookups = [
            FeatureLookup(
                table_name=f"{self.config.CATALOG}.{self.config.SCHEMA}.lojas_fs",
                lookup_key=["codigo_loja"],
                feature_names=["cluster_loja", "sigla_uf", "tipo_loja", "modelo_loja"]
            ),
            FeatureLookup(
                table_name=f"{self.config.CATALOG}.{self.config.SCHEMA}.historico_feriados_loja",
                lookup_key=["codigo_loja"],
                timestamp_lookup_key="data", # Importante: Join considerando o tempo
                feature_names=["valor"], 
                rename_outputs={"valor": "is_feriado"}
            )
        ]

        # Executa o join inteligente do Feature Store
        training_set = self.fe.create_training_set(
            df=df_spine,
            feature_lookups=feature_lookups,
            label="target_vendas",
            exclude_columns=[]
        )

        df_spark = training_set.load_df()

        print("   ‚ö° Executando limpeza e tratamento no Spark Cluster...")
        # Preenche valores nulos que podem ter surgido do Join (ex: loja sem feriado na data)
        df_spark = df_spark.na.fill({
            "is_feriado": 0.0, 
            "target_vendas": 0.0,
            "cluster_loja": "DESCONHECIDO",
            "sigla_uf": "DESCONHECIDO",
            "tipo_loja": "DESCONHECIDO",
            "modelo_loja": "DESCONHECIDO"
        })

        # Garante tipagem correta da data
        df_spark = df_spark.withColumn("data", F.to_timestamp("data"))
        return df_spark

    def get_global_support(self) -> pd.DataFrame:
        """
        Carrega s√©ries temporais globais (n√£o espec√≠ficas de loja), como indicadores econ√¥micos agregados.
        Estas s√©ries ajudam o modelo a entender tend√™ncias macro.
        
        Returns:
            pd.DataFrame: DataFrame Pandas indexado por dia, preenchido para dias faltantes.
        """
        table_name = "historico_suporte_loja"
        print(f"üåç Carregando suporte global (Spark Aggregation)...")
        # Agrega metricas globais por dia
        df_spark = (self.spark.table(f"{self.config.CATALOG}.{self.config.SCHEMA}.{table_name}")
            .filter(F.col("DATA").between(self.config.DATA_START, self.config.INGESTION_END))
            .groupBy("data")
            .pivot("metricas")
            .agg(F.sum("valor"))
            .na.fill(0.0)
        )
        pdf = df_spark.toPandas()
        pdf['data'] = pd.to_datetime(pdf['data'])
        # Garante frequ√™ncia di√°ria, preenchendo buracos com 0
        return pdf.set_index('data').asfreq('D').fillna(0.0)

    def build_darts_objects(
        self, 
        df_spark_wide: DataFrame, 
        df_global_support: pd.DataFrame, 
        df_market_indicators: Optional[pd.DataFrame] = None
    ) -> Tuple[List[TimeSeries], List[TimeSeries]]:
        """
        Converte os dados tabulares (Spark DataFrame) para a estrutura de objetos do Darts.
        
        O Darts exige objetos `TimeSeries`. Para modelos globais (treinar 1 modelo para N lojas),
        precisamos de uma lista de TimeSeries, uma para cada loja.
        
        Args:
            df_spark_wide: DataFrame principal com Vendas e Atributos da loja.
            df_global_support: Dados agregados globais.
            df_market_indicators: (Opcional) Outros indicadores de mercado externos.
            
        Returns:
            Tuple: (Lista de S√©ries Alvo [Vendas], Lista de S√©ries de Covari√°veis [Feriados + Globais])
        """
        print("‚öôÔ∏è Materializando dados do Spark para Pandas (Driver)...")
        # CUIDADO: Trazendo dados para a mem√≥ria do Driver.
        # Para datasets massivos, considerar processamento distribu√≠do (Fugue/PandasUDF), 
        # mas para s√©ries agregadas por loja, geralmente cabe na mem√≥ria.
        df_wide = df_spark_wide.toPandas()
        
        print(f"   DEBUG: Columns before dedupe: {list(df_wide.columns)}")
        # Remove colunas duplicadas (pode acontecer se o join trouxe chaves repetidas)
        df_wide = df_wide.loc[:, ~df_wide.columns.duplicated()]
        print(f"   DEBUG: Columns after dedupe: {list(df_wide.columns)}")
        
        # Verifica√ß√£o cr√≠tica de tipagem da coluna loja
        if "codigo_loja" in df_wide.columns:
             col_obj = df_wide["codigo_loja"]
             # Se por algum motivo 'codigo_loja' ainda for um DataFrame (duplicidade extrema), for√ßamos uma corre√ß√£o.
             if isinstance(col_obj, pd.DataFrame):
                  print("   ‚ö†Ô∏è CRITICAL: 'codigo_loja' is still a DataFrame (duplicate columns)!")
                  df_wide = df_wide.loc[:, ~df_wide.columns.duplicated(keep='first')]

        if df_wide.empty:
            print("‚ö†Ô∏è AVISO: DataFrame df_wide est√° vazio! Verifique os filtros de data e dados.")
            return [], []

        df_wide['data'] = pd.to_datetime(df_wide['data'])
        
        # Defini√ß√£o das colunas que s√£o est√°ticas (n√£o mudam com o tempo para uma mesma loja)
        possible_static = ["cluster_loja", "sigla_uf", "tipo_loja", "modelo_loja"]
        static_cols = [c for c in possible_static if c in df_wide.columns]

        # --- CRIA√á√ÉO DAS TARGET SERIES (O que queremos prever) ---
        print("   Build: Criando Target Series (Vetorizado)...")
        try:
            # from_group_dataframe √© o m√©todo mais eficiente para criar m√∫ltiplas s√©ries de um DF longo
            target_series_list = TimeSeries.from_group_dataframe(
                df_wide,
                group_cols="codigo_loja",
                time_col="data",
                value_cols="target_vendas",
                static_cols=static_cols, # Associa features de loja como covari√°veis est√°ticas
                freq='D',
                fill_missing_dates=True,
                fillna_value=0.0
            )
        except Exception as e:
            print(f"‚ùå Erro cr√≠tico no from_group_dataframe (Target): {e}")
            raise e

        # Mapa para acesso r√°pido (ID Loja -> TimeSeries)
        target_dict = {}
        for ts in target_series_list:
            if ts.static_covariates is not None and not ts.static_covariates.empty:
                # Ajuste t√©cnico: Garantir que o √≠ndice da covari√°vel est√°tica tenha nome
                if ts.static_covariates.index.name == "target_vendas":
                     ts.with_static_covariates(ts.static_covariates.rename_axis("codigo_loja"))
                
                # Extrai ID da loja da covari√°vel est√°tica (Darts coloca o ID do grupo l√°)
                key_val = str(ts.static_covariates.index[0]).replace(".0", "")
                target_dict[key_val] = ts
        
        valid_stores = list(target_dict.keys())
        print(f"   ‚ÑπÔ∏è Lojas identificadas: {len(valid_stores)}")

        # --- CRIA√á√ÉO DAS COVARI√ÅVEIS LOCAIS (Feriados) ---
        print("   Build: Criando Covari√°veis Locais...")
        try:
            # Covari√°veis passadas/futuras conhecidas espec√≠ficas por loja
            feriado_series_list = TimeSeries.from_group_dataframe(
                df_wide,
                group_cols="codigo_loja",
                time_col="data",
                value_cols="is_feriado",
                static_cols=static_cols,
                freq='D',
                fill_missing_dates=True,
                fillna_value=0.0
            )
        except Exception as e:
            print(f"‚ùå Erro cr√≠tico no from_group_dataframe (Feriado): {e}")
            raise e

        feriado_dict = {
            str(ts.static_covariates.index[0]).replace(".0", ""): ts 
            for ts in feriado_series_list
            if ts.static_covariates is not None and not ts.static_covariates.empty
        }

        # --- PREPARA√á√ÉO DE COVARI√ÅVEIS GLOBAIS ---
        print("   Build: Preparando Covari√°veis Globais...")
        ts_support = TimeSeries.from_dataframe(df_global_support, fill_missing_dates=True, freq='D', fillna_value=0.0)
        
        # Combina (stack) suporte global com indicadores de mercado (se houver)
        if df_market_indicators is not None:
             ts_market = TimeSeries.from_dataframe(df_market_indicators, fill_missing_dates=True, freq='D', fillna_value=0.0)
             global_covariates = ts_support.stack(ts_market)
        else:
             global_covariates = ts_support

        # Features de Calend√°rio (Sazonalidade)
        # Adiciona dia da semana, trimestre, semana do ano, etc.
        ts_time = datetime_attribute_timeseries(df_global_support.index, attribute="dayofweek", cyclic=True)
        ts_time = ts_time.stack(datetime_attribute_timeseries(df_global_support.index, attribute="quarter", one_hot=True))
        ts_time = ts_time.stack(datetime_attribute_timeseries(df_global_support.index, attribute="week", cyclic=True))
        
        # Stack Final Global: Suporte + Mercado + Calend√°rio
        global_covariates = global_covariates.stack(ts_time)

        final_target_list = []
        full_covariates_list = []

        print("   Build: Stacking Final (Otimizado)...")
        # Combina Covari√°veis GLOBAIS + LOCAIS para cada loja
        for loja in valid_stores:
            ts_target = target_dict[loja]
            final_target_list.append(ts_target)
            
            ts_local = feriado_dict.get(loja)
            
            # Se n√£o existir feriado para a loja (caso raro), cria zerado
            if ts_local is None:
                ts_local = TimeSeries.from_times_and_values(
                    ts_target.time_index, 
                    np.zeros((len(ts_target), 1)), 
                    freq='D',
                    columns=["is_feriado"]
                )
            else:
                # Garante sincronia temporal (interse√ß√£o)
                if ts_local.start_time() != ts_target.start_time() or ts_local.end_time() != ts_target.end_time():
                    ts_local = ts_local.slice_intersect(ts_target)

            # Sincroniza globais com o per√≠odo da loja
            if global_covariates.start_time() != ts_target.start_time() or global_covariates.end_time() != ts_target.end_time():
                 ts_global_cut = global_covariates.slice_intersect(ts_target)
            else:
                 ts_global_cut = global_covariates

            # STACK: Une Global + Local em uma √∫nica s√©rie multivalorada de features
            full_covariates_list.append(ts_global_cut.stack(ts_local))

        print(f"‚úÖ Objetos Darts Prontos: {len(final_target_list)} lojas processadas.")
        return final_target_list, full_covariates_list