"""
M√≥dulo de Feature Store (Ingestion).

Este arquivo cont√©m a l√≥gica para persistir dados transformados no Databricks Feature Store.
Ele gerencia a cria√ß√£o e atualiza√ß√£o de Feature Tables, garantindo integridade de dados 
(chaves prim√°rias √∫nicas e n√£o nulas) e performance (Liquid Clustering/Optimize).

Fun√ß√µes:
- salvar_feature_table: Fun√ß√£o principal para gravar DataFrames como tabelas do Feature Store.
"""

from typing import List, Union, Optional
from databricks.feature_engineering import FeatureEngineeringClient
from pyspark.sql import SparkSession, DataFrame

def salvar_feature_table(
    df: DataFrame, 
    table_name_full: str, 
    pk_columns: Union[str, List[str]], 
    timestamp_col: Optional[str] = None, 
    spark: Optional[SparkSession] = None
) -> None:
    """
    Salva ou atualiza uma tabela no Feature Store aplicando melhores pr√°ticas de Engenharia de Dados.
    
    Esta fun√ß√£o realiza v√°rias etapas cr√≠ticas:
    1. Valida√ß√£o e limpeza de chaves prim√°rias (PKs) e Timestamp.
    2. Remo√ß√£o de duplicatas para garantir integridade.
    3. Tentativa de 'merge' (upsert) se a tabela j√° existir.
    4. Cria√ß√£o de nova tabela com Liquid Clustering se n√£o existir.
    5. Otimiza√ß√£o de armazenamento (Optimize/Vacuum) para performance de leitura.

    Args:
        df (DataFrame): O DataFrame PySpark com os dados a serem ingeridos.
        table_name_full (str): Nome completo da tabela de destino (ex: catalog.schema.table).
        pk_columns (Union[str, List[str]]): Nome da(s) coluna(s) que identificam unicamente cada registro.
        timestamp_col (Optional[str]): Coluna de tempo para permitir "Point-in-time lookup" (evita data leakage).
        spark (Optional[SparkSession]): Sess√£o Spark ativa. Se None, cria/obt√©m uma nova.

    Returns:
        None: A fun√ß√£o realiza opera√ß√µes de efeito colateral (grava√ß√£o no banco).
    """
    if spark is None:
        spark = SparkSession.builder.getOrCreate()
        
    # Cliente do Feature Engineering para interagir com o cat√°logo de features
    fe = FeatureEngineeringClient()

    # 1. Normaliza√ß√£o de Inputs
    # Garante que pk_columns seja sempre uma lista, mesmo que venha como string √∫nica
    if isinstance(pk_columns, str):
        pk_columns = [pk_columns]

    # Cria lista de verifica√ß√£o (todas as colunas que n√£o podem ser nulas)
    check_keys = pk_columns.copy()
    
    # Regra do Feature Store: Se a tabela tem uma dimens√£o de tempo (timestamp),
    # essa coluna tamb√©m faz parte da unicidade do registro para lookups hist√≥ricos.
    if timestamp_col:
        if timestamp_col not in pk_columns:
            pk_columns.append(timestamp_col)
        if timestamp_col not in check_keys:
            check_keys.append(timestamp_col)

    # --- LIMPEZA DE DADOS (DATA QUALITY) ---
    
    # Valida√ß√£o 1: Chaves Prim√°rias no Feature Store N√ÉO podem ser nulas.
    # Se houver nulos, a grava√ß√£o falharia com erro "NOT NULL constraint violated".
    print(f"   üßπ Removendo Nulos nas chaves: {check_keys}...")
    df = df.dropna(subset=check_keys)

    # Valida√ß√£o 2: Unicidade.
    # O Feature Store exige que cada combina√ß√£o de PK+Timestamp seja √∫nica.
    # Removemos duplicatas arbitr√°rias (primeira ocorr√™ncia vence) para evitar falhas.
    print(f"   üßπ Removendo duplicatas nas chaves: {check_keys}...")
    df = df.dropDuplicates(check_keys)

    # 2. Estrat√©gia de Grava√ß√£o (Merge vs Create)
    try:
        # Verifica se a tabela j√° existe e √© uma Feature Table v√°lida
        fe.get_table(name=table_name_full)
        print(f"üîÑ [UPDATE] Tabela encontrada no Feature Store: {table_name_full}")
        
        # Realiza um MERGE (Upsert): Atualiza registros existentes e insere novos
        fe.write_table(
            name=table_name_full,
            df=df,
            mode="merge"
        )
        
        # --- OTIMIZA√á√ÉO DE STORAGE ---
        # OPTIMIZE: Condensa pequenos arquivos em arquivos maiores (melhora leitura).
        # VACUUM: Remove arquivos antigos n√£o mais referenciados pelo log transacional (economiza espa√ßo).
        print(f"   ‚ö° Otimizando a tabela (Liquid/Z-Order + Compacta√ß√£o)...")
        spark.sql(f"OPTIMIZE {table_name_full}")
        spark.sql(f"VACUUM {table_name_full} RETAIN 168 HOURS") # Mant√©m hist√≥rico de 7 dias para Time Travel
        
    except Exception:
        # Se ocorrer erro no get_table, assumimos que a tabela n√£o existe ou n√£o est√° configurada corretamente.
        
        # Cleanup preventivo: Se existir como tabela Delta comum (mas n√£o Feature Table), removemos para recriar do zero.
        if spark.catalog.tableExists(table_name_full):
            print(f"‚ö†Ô∏è [CLEANUP] Tabela existe mas sem restri√ß√µes de Feature Store. Removendo: {table_name_full}")
            spark.sql(f"DROP TABLE IF EXISTS {table_name_full}")
            
        print(f"üÜï [CREATE] Criando nova Feature Table: {table_name_full}")
        print(f"   üîë PKs: {pk_columns} | üïí Time: {timestamp_col}")
        
        # Cria√ß√£o da tabela com suporte a features
        # Nota: Liquid Clustering (se suportado pela vers√£o) √© a melhor pr√°tica atual para particionamento.
        fe.create_table(
            name=table_name_full,
            primary_keys=pk_columns,
            timestamp_keys=timestamp_col,
            df=df,
            schema=df.schema,
            description="Ingested via JDBC for Feature Store"
            # table_properties={"delta.enableLiquidClustering": "true"} # Descomentar se ambiente suportar SDK compat√≠vel
        )
        
        # Garante otimiza√ß√£o do layout inicial dos arquivos
        print(f"   ‚ö° Otimizando layout inicial...")
        spark.sql(f"OPTIMIZE {table_name_full}")
