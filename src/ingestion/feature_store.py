from typing import List, Union, Optional
from databricks.feature_engineering import FeatureEngineeringClient
from pyspark.sql import SparkSession, DataFrame

def salvar_feature_table(
    df: DataFrame, 
    table_name_full: str, 
    pk_columns: Union[str, List[str]], 
    timestamp_col: Optional[str] = None, 
    spark: Optional[SparkSession] = None
) -> None:
    """
    Salva ou atualiza uma tabela no Feature Store com melhores pr√°ticas (Liquid Clustering/Optimize).

    Args:
        df (DataFrame): O DataFrame PySpark contendo os features.
        table_name_full (str): Nome completo da tabela (catalog.schema.table).
        pk_columns (Union[str, List[str]]): Coluna(s) chave prim√°ria.
        timestamp_col (Optional[str]): Coluna de timestamp para Point-in-time lookup.
        spark (Optional[SparkSession]): Sess√£o Spark ativa.

    Returns:
        None
    """
    if spark is None:
        spark = SparkSession.builder.getOrCreate()
        
    fe = FeatureEngineeringClient()

    # 1. Normaliza√ß√£o de Inputs
    if isinstance(pk_columns, str):
        pk_columns = [pk_columns]

    # Cria lista de verifica√ß√£o (PKs + Timestamp se houver)
    check_keys = pk_columns.copy()
    
    # Regra do Feature Store: Se tem timestamp, ele DEVE estar na lista de PKs
    if timestamp_col:
        if timestamp_col not in pk_columns:
            pk_columns.append(timestamp_col)
        if timestamp_col not in check_keys:
            check_keys.append(timestamp_col)

    # --- CORRE√á√ÉO 1: REMOVER NULOS ---
    # Chaves Prim√°rias no Feature Store N√ÉO podem ser nulas.
    # O erro "NOT NULL constraint violated" acontece aqui se n√£o limparmos.
    print(f"   üßπ Removendo Nulos nas chaves: {check_keys}...")
    df = df.dropna(subset=check_keys)

    # --- CORRE√á√ÉO 2: REMOVER DUPLICATAS ---
    # Garante unicidade
    print(f"   üßπ Removendo duplicatas nas chaves: {check_keys}...")
    df = df.dropDuplicates(check_keys)

    # 2. Tentativa de Atualiza√ß√£o ou Cria√ß√£o Limpa
    try:
        # Tenta carregar a tabela como Feature Table
        fe.get_table(name=table_name_full)
        print(f"üîÑ [UPDATE] Tabela encontrada no Feature Store: {table_name_full}")
        
        fe.write_table(
            name=table_name_full,
            df=df,
            mode="merge"
        )
        
        # --- BEST PRACTICE: OTIMIZA√á√ÉO CONT√çNUA ---
        print(f"   ‚ö° Otimizando a tabela (Liquid/Z-Order + Compacta√ß√£o)...")
        spark.sql(f"OPTIMIZE {table_name_full}")
        spark.sql(f"VACUUM {table_name_full} RETAIN 168 HOURS") # Limpa arquivos antigos (>7 dias)
        
    except Exception:
        # Se cair aqui, verifica se tabela existe como Delta comum e remove
        if spark.catalog.tableExists(table_name_full):
            print(f"‚ö†Ô∏è [CLEANUP] Tabela existe mas sem restri√ß√µes de Feature Store. Removendo: {table_name_full}")
            spark.sql(f"DROP TABLE IF EXISTS {table_name_full}")
            
        print(f"üÜï [CREATE] Criando nova Feature Table: {table_name_full}")
        print(f"   üîë PKs: {pk_columns} | üïí Time: {timestamp_col}")
        
        # --- BEST PRACTICE: LIQUID CLUSTERING (Recomendado pela Databricks ao inv√©s de Z-Order/Partition) ---
        fe.create_table(
            name=table_name_full,
            primary_keys=pk_columns,
            timestamp_keys=timestamp_col,
            df=df,
            schema=df.schema,
            description="Ingested via JDBC for Feature Store"
            # table_properties={"delta.enableLiquidClustering": "true"} # REMOVED: SDK compatibility issue
        )
        
        # Garante otimiza√ß√£o inicial
        print(f"   ‚ö° Otimizando layout inicial...")
        spark.sql(f"OPTIMIZE {table_name_full}")
