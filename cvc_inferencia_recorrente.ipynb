{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff287ba8-3984-43a3-84f5-180cf0709723",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# üîÑ Infer√™ncia Recorrente (Batch Scoring) ‚Äî CVC Lojas\n",
    "\n",
    "## üéØ Objetivo\n",
    "Gerar previs√µes de vendas para os pr√≥ximos dias usando o modelo produtivo `Champion`.\n",
    "\n",
    "## ‚öôÔ∏è Fluxo de Execu√ß√£o\n",
    "1. **Context Loading:** Carrega hist√≥rico recente via `DataIngestion` (Feature Store) ‚Äî **mesmo padr√£o do treino**.\n",
    "2. **Market Data:** Carrega `historico_suporte_loja` (IPCA, D√≥lar, etc.) com cobertura futura.\n",
    "3. **Model Loading:** Baixa o modelo `Champion` do Unity Catalog.\n",
    "4. **Inference:** Envia o DataFrame cru ao `UnifiedForecaster`. O wrapper aplica internamente: extens√£o de datas, features de calend√°rio e scaling.\n",
    "5. **Persistence:** Salva os resultados na tabela `previsao_lojas_futuro`.\n",
    "\n",
    "---\n",
    "**‚ö†Ô∏è IMPORTANTE:** Este notebook N√ÉO gera features de calend√°rio manualmente.\n",
    "O `UnifiedForecaster` (wrapper) j√° faz isso internamente via `_add_calendar_features()`,\n",
    "garantindo total consist√™ncia com o padr√£o usado no treinamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d14b5890-6b94-43bb-b975-a3792429d02a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 0. SETUP E IMPORTS\n",
    "# ==============================================================================\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.getcwd())\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mlflow\n",
    "import pyspark.sql.functions as F\n",
    "from datetime import date, timedelta, datetime\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "# M√≥dulos do projeto\n",
    "from src.validation.config import Config\n",
    "from src.validation.data import DataIngestion\n",
    "\n",
    "# Configs Spark\n",
    "spark.conf.set(\"spark.databricks.delta.optimizeWrite.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.databricks.delta.autoCompact.enabled\", \"true\")\n",
    "\n",
    "client = MlflowClient()\n",
    "print(\"‚úÖ Setup conclu√≠do.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4922c5a6-a6c4-4c2c-899b-58698ae43935",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 1. CONFIGURA√á√ÉO DA JANELA DE CONTEXTO\n",
    "# ==============================================================================\n",
    "# Horizonte de previs√£o (em dias)\n",
    "FORECAST_HORIZON = 35\n",
    "\n",
    "# 'Hoje' ‚Äî em produ√ß√£o, usar date.today()\n",
    "today = date.today()\n",
    "\n",
    "# Janela de lookback: suficiente para os lags do modelo (max_lag=15) + margem\n",
    "# O wrapper usa max_lag=15 por padr√£o. Usamos 90 dias para ter hist√≥rico farto.\n",
    "context_days = 90\n",
    "start_context = today - timedelta(days=context_days)\n",
    "\n",
    "# Config din√¢mica (mesmos par√¢metros do treino)\n",
    "config = Config(spark)\n",
    "config.DATA_START  = start_context.strftime(\"%Y-%m-%d\")\n",
    "config.INGESTION_END = today.strftime(\"%Y-%m-%d\")\n",
    "config.SCHEMA = \"cvc_pred\"\n",
    "\n",
    "print(f\"üìÖ Data de Refer√™ncia (Hoje): {today}\")\n",
    "print(f\"üîé Janela de contexto: {config.DATA_START} ‚Üí {config.INGESTION_END}\")\n",
    "print(f\"üîÆ Horizonte de previs√£o: {FORECAST_HORIZON} dias\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa8f1a9a-ddeb-4ef2-ad92-c5b3dd0735ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 2. CARREGAMENTO DO HIST√ìRICO VIA FEATURE STORE\n",
    "#    Usa exatamente o mesmo DataIngestion do treino ‚Äî garante consist√™ncia\n",
    "#    nas features est√°ticas (cluster_loja, sigla_uf, tipo_loja, modelo_loja)\n",
    "#    e covari√°veis locais (is_feriado).\n",
    "# ==============================================================================\n",
    "print(\"‚è≥ Carregando hist√≥rico recente via Feature Store...\")\n",
    "ingestion = DataIngestion(spark, config)\n",
    "df_spark_raw = ingestion.create_training_set()\n",
    "\n",
    "# Filtro de seguran√ßa (a DataIngestion j√° filtra, mas garantimos aqui)\n",
    "df_spark_raw = df_spark_raw.filter(\n",
    "    F.col(\"data\").between(config.DATA_START, config.INGESTION_END)\n",
    ")\n",
    "\n",
    "# Converte para Pandas ‚Äî volume pequeno (janela de 90 dias)\n",
    "df_context_pd = df_spark_raw.toPandas()\n",
    "df_context_pd['data'] = pd.to_datetime(df_context_pd['data'])\n",
    "df_context_pd['codigo_loja'] = (\n",
    "    df_context_pd['codigo_loja']\n",
    "    .astype(str)\n",
    "    .str.replace(r'\\.0$', '', regex=True)\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Hist√≥rico carregado: {len(df_context_pd)} linhas | {df_context_pd['codigo_loja'].nunique()} lojas\")\n",
    "print(f\"   Colunas: {df_context_pd.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b059bc0b-85c2-46a8-8ea6-27220dd027c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 3. CARREGAMENTO DOS INDICADORES DE MERCADO (historico_suporte_loja)\n",
    "#    Segue o mesmo padr√£o de get_global_support() do data.py:\n",
    "#    - Sem filtro de data (carrega tudo para ter cobertura futura)\n",
    "#    - Pivot por 'metricas'\n",
    "#    - Frequ√™ncia di√°ria cont√≠nua (asfreq + ffill)\n",
    "#    - Extens√£o para cobrir o FORECAST_HORIZON\n",
    "# ==============================================================================\n",
    "print(\"üìä Carregando indicadores de mercado (suporte global)...\")\n",
    "\n",
    "df_market_spark = (\n",
    "    spark.table(f\"{config.CATALOG}.{config.SCHEMA}.historico_suporte_loja\")\n",
    "    .groupBy(\"data\")\n",
    "    .pivot(\"metricas\")\n",
    "    .agg(F.sum(\"valor\"))\n",
    "    .na.fill(0.0)\n",
    ")\n",
    "\n",
    "pdf_market = df_market_spark.toPandas()\n",
    "pdf_market['data'] = pd.to_datetime(pdf_market['data'])\n",
    "\n",
    "# Garante frequ√™ncia di√°ria cont√≠nua (igual ao data.py)\n",
    "pdf_market = (\n",
    "    pdf_market\n",
    "    .set_index('data')\n",
    "    .asfreq('D')\n",
    "    .fillna(0.0)\n",
    ")\n",
    "\n",
    "# Extens√£o futura: cobre o horizonte de previs√£o (igual ao get_global_support)\n",
    "full_market_range = pd.date_range(\n",
    "    start=pdf_market.index.min(),\n",
    "    periods=len(pdf_market) + FORECAST_HORIZON + 15,  # margem extra\n",
    "    freq='D'\n",
    ")\n",
    "pdf_market = pdf_market.reindex(full_market_range).ffill().fillna(0.0).reset_index()\n",
    "pdf_market.rename(columns={'index': 'data'}, inplace=True)\n",
    "\n",
    "market_cols = [c for c in pdf_market.columns if c != 'data']\n",
    "print(f\"‚úÖ Mercado carregado: {len(pdf_market)} dias | Indicadores: {market_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "832f7d90-9880-4b38-b920-1cf90a13d4af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 4. MONTAGEM DO DATAFRAME DE INFER√äNCIA\n",
    "#    Estrat√©gia:\n",
    "#    a) Expande o hist√≥rico por loja √ó datas (hist√≥rico + futuro)\n",
    "#    b) Faz merge com os indicadores de mercado\n",
    "#    c) N√ÉO gera features de calend√°rio aqui ‚Äî o UnifiedForecaster.predict()\n",
    "#       chama _add_calendar_features() internamente (dayofweek, quarter, week)\n",
    "#       garantindo exatamente o mesmo padr√£o do treinamento.\n",
    "# ==============================================================================\n",
    "print(\"üîß Montando DataFrame de infer√™ncia...\")\n",
    "\n",
    "last_date_history = df_context_pd['data'].max()\n",
    "start_date_context = df_context_pd['data'].min()\n",
    "\n",
    "# Margem de 3 meses para tr√°s (igual ao covariates_range do data.py)\n",
    "safe_start_date = start_date_context - pd.DateOffset(months=3)\n",
    "\n",
    "# Datas futuras: hist√≥rico + horizonte + buffer para os lags\n",
    "future_end = last_date_history + pd.Timedelta(days=FORECAST_HORIZON + 15)\n",
    "inference_range = pd.date_range(start=safe_start_date, end=future_end, freq='D')\n",
    "\n",
    "# Colunas est√°ticas dispon√≠veis (provenientes do Feature Store)\n",
    "static_cols = ['codigo_loja', 'cluster_loja', 'sigla_uf', 'tipo_loja', 'modelo_loja']\n",
    "static_cols = [c for c in static_cols if c in df_context_pd.columns]\n",
    "\n",
    "# Um registro por loja com os atributos est√°ticos mais recentes\n",
    "df_stores_ref = (\n",
    "    df_context_pd\n",
    "    .sort_values('data')\n",
    "    .groupby('codigo_loja')[static_cols]\n",
    "    .tail(1)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# Cross-join: loja √ó todas as datas do intervalo\n",
    "df_full_timeline = (\n",
    "    df_stores_ref\n",
    "    .assign(key=1)\n",
    "    .merge(pd.DataFrame({'data': inference_range, 'key': 1}), on='key')\n",
    "    .drop('key', axis=1)\n",
    ")\n",
    "\n",
    "# Salvar as ventas reais (target) ‚Äî futuro fica NaN (o wrapper sabe lidar)\n",
    "df_full_timeline = pd.merge(\n",
    "    df_full_timeline,\n",
    "    df_context_pd[['data', 'codigo_loja', 'target_vendas']],\n",
    "    on=['data', 'codigo_loja'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Feriados: carregados da tabela oficial (cobre passado + futuro)\n",
    "print(\"üóìÔ∏è Carregando calend√°rio de feriados...\")\n",
    "df_feriados_pd = (\n",
    "    spark.table(f\"{config.CATALOG}.{config.SCHEMA}.historico_feriados_loja\")\n",
    "    .select(\"codigo_loja\", \"data\", \"valor\")\n",
    "    .withColumn(\"codigo_loja\", F.col(\"codigo_loja\").cast(\"string\"))\n",
    "    .toPandas()\n",
    ")\n",
    "df_feriados_pd['data'] = pd.to_datetime(df_feriados_pd['data'])\n",
    "df_feriados_pd['codigo_loja'] = (\n",
    "    df_feriados_pd['codigo_loja']\n",
    "    .astype(str)\n",
    "    .str.replace(r'\\.0$', '', regex=True)\n",
    ")\n",
    "df_feriados_pd.rename(columns={'valor': 'is_feriado'}, inplace=True)\n",
    "\n",
    "# Merge de feriados\n",
    "df_full_timeline = pd.merge(\n",
    "    df_full_timeline,\n",
    "    df_feriados_pd,\n",
    "    on=['data', 'codigo_loja'],\n",
    "    how='left'\n",
    ")\n",
    "df_full_timeline['is_feriado'] = df_full_timeline['is_feriado'].fillna(0.0)\n",
    "# target_vendas: NaN nas datas futuras √© intencional (wrapper usa para separar hist√≥rico)\n",
    "\n",
    "# Merge com indicadores de mercado\n",
    "df_inference_final = pd.merge(df_full_timeline, pdf_market, on='data', how='left')\n",
    "df_inference_final[market_cols] = df_inference_final[market_cols].ffill().bfill().fillna(0.0)\n",
    "\n",
    "# Ajuste de tipos\n",
    "for col in market_cols + ['is_feriado']:\n",
    "    df_inference_final[col] = df_inference_final[col].astype(float)\n",
    "\n",
    "# Coluna 'n': horizonte de previs√£o para o wrapper\n",
    "df_inference_final['n'] = int(FORECAST_HORIZON)\n",
    "\n",
    "# Data como string (exig√™ncia do schema MLflow)\n",
    "df_inference_final['data'] = df_inference_final['data'].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "print(f\"‚úÖ DataFrame de infer√™ncia montado.\")\n",
    "print(f\"   Linhas: {len(df_inference_final)} | Colunas: {len(df_inference_final.columns)}\")\n",
    "print(f\"   Colunas: {df_inference_final.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a35f23fc-6484-4b96-b048-03f40ef90a20",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 5. CARREGAMENTO DO MODELO CHAMPION\n",
    "# ==============================================================================\n",
    "model_name = f\"{config.CATALOG}.cvc_pred.cvc_lojas_forecast_production\"\n",
    "\n",
    "print(f\"üì¶ Carregando modelo: {model_name}@Champion\")\n",
    "loaded_model = mlflow.pyfunc.load_model(f\"models:/{model_name}@Champion\")\n",
    "\n",
    "mv = client.get_model_version_by_alias(name=model_name, alias=\"Champion\")\n",
    "print(f\"‚úÖ Modelo carregado!\")\n",
    "print(f\"   Vers√£o : {mv.version}\")\n",
    "print(f\"   Run ID : {mv.run_id}\")\n",
    "print(f\"   Desc   : {mv.description}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d7cf4c1-beb8-4a03-86f2-019fbd283356",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 6. DIAGN√ìSTICO PR√â-INFER√äNCIA\n",
    "#    Verifica o metadata do modelo carregado para garantir alinhamento de\n",
    "#    colunas entre treino e infer√™ncia.\n",
    "# ==============================================================================\n",
    "try:\n",
    "    python_model = loaded_model._model_impl.python_model\n",
    "    meta = getattr(python_model, 'metadata', {})\n",
    "    if meta:\n",
    "        static_order = meta.get('static_cols_order', [])\n",
    "        cov_order    = meta.get('covariate_cols_order', [])\n",
    "        max_lag      = meta.get('max_lag', 'N/A')\n",
    "        print(f\"üßê Metadata do Modelo:\")\n",
    "        print(f\"   static_cols_order    : {static_order}\")\n",
    "        print(f\"   covariate_cols_order : {cov_order}\")\n",
    "        print(f\"   max_lag              : {max_lag}\")\n",
    "\n",
    "        # Verifica se todas as covari√°veis do treino est√£o presentes no input\n",
    "        missing_covs = [c for c in cov_order if c not in df_inference_final.columns]\n",
    "        if missing_covs:\n",
    "            print(f\"‚ö†Ô∏è ATEN√á√ÉO ‚Äî Covari√°veis do treino ausentes no input: {missing_covs}\")\n",
    "            print(\"   Preenchendo com zeros...\")\n",
    "            for mc in missing_covs:\n",
    "                df_inference_final[mc] = 0.0\n",
    "        else:\n",
    "            print(\"‚úÖ Todas as covari√°veis do treino est√£o presentes no input.\")\n",
    "    else:\n",
    "        print(\"‚ÑπÔ∏è Metadata n√£o encontrado ‚Äî wrapper usar√° heur√≠stica de fallback.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è N√£o foi poss√≠vel ler o metadata do modelo: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b56cf72d-2dc4-4f8c-8ace-6d9505578e82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 7. INFER√äNCIA\n",
    "#    O UnifiedForecaster recebe o DataFrame cru e internamente:\n",
    "#    a) Expande datas futuras (_ensure_future_horizon)\n",
    "#    b) Gera features de calend√°rio (_add_calendar_features) ‚Äî dayofweek, quarter, week\n",
    "#    c) Constr√≥i objetos TimeSeries Darts com as colunas ordenadas pelo metadata\n",
    "#    d) Aplica o pipeline de scaling (target_pipeline, static_pipeline, covariate_pipeline)\n",
    "#    e) Chama model.predict() e inverte o scaling\n",
    "# ==============================================================================\n",
    "print(\"üîÆ Gerando previs√µes...\")\n",
    "\n",
    "# Saneamento at√¥mico: garante que n√£o haja colunas duplicadas (artefato de merges)\n",
    "clean_dict = {}\n",
    "for col in df_inference_final.columns.unique():\n",
    "    col_name = str(col).strip()\n",
    "    series_data = df_inference_final[col]\n",
    "    if isinstance(series_data, pd.DataFrame):\n",
    "        series_data = series_data.iloc[:, 0]\n",
    "    clean_dict[col_name] = series_data.values.flatten()\n",
    "\n",
    "df_inference_cleaned = pd.DataFrame(clean_dict)\n",
    "\n",
    "# Predi√ß√£o ‚Äî o wrapper cuida de todo o pr√©-processamento internamente\n",
    "forecast_df = loaded_model.predict(df_inference_cleaned)\n",
    "\n",
    "# Metadados de rastreabilidade\n",
    "forecast_df['version_model']     = mv.version\n",
    "forecast_df['description_model'] = mv.description\n",
    "forecast_df['model_name']        = model_name\n",
    "forecast_df['data_reference']    = datetime.now()\n",
    "\n",
    "n_lojas = forecast_df['codigo_loja'].nunique()\n",
    "print(f\"‚úÖ Previs√£o conclu√≠da para {n_lojas} lojas.\")\n",
    "print(f\"   Per√≠odo previsto: {forecast_df['data_previsao'].min()} ‚Üí {forecast_df['data_previsao'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ec49d32-c650-43f6-a25f-3f6fd5c733a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 8. PERSIST√äNCIA (WRITE BACK)\n",
    "# ==============================================================================\n",
    "output_table = f\"{config.CATALOG}.{config.SCHEMA}.previsao_lojas_futuro\"\n",
    "\n",
    "print(f\"üíæ Salvando resultados em: {output_table}\")\n",
    "(\n",
    "    spark.createDataFrame(forecast_df)\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"append\")\n",
    "    .option(\"mergeSchema\", \"true\")\n",
    "    .saveAsTable(output_table)\n",
    ")\n",
    "spark.sql(f\"OPTIMIZE {output_table}\")\n",
    "print(\"‚ú® Sucesso! Dados salvos e otimizados.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d7cf4c1-beb8-4a03-86f2-019fbd283357",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(forecast_df)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "cvc_inferencia_recorrente",
   "widgets": {
    "catalog": {
     "currentValue": "ds_dev",
     "nuid": "4baaa425-c6d7-4b26-8cbe-ca720f35b0af",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "ds_dev",
      "label": "4. Cat√°logo",
      "name": "catalog",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "ds_dev",
      "label": "4. Cat√°logo",
      "name": "catalog",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "forecast_horizon": {
     "currentValue": "35",
     "nuid": "4b2598a8-328e-41df-9cc4-a1de1b865d04",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "35",
      "label": "5. Horizonte (Dias)",
      "name": "forecast_horizon",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "35",
      "label": "5. Horizonte (Dias)",
      "name": "forecast_horizon",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}