{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# \ud83d\udd04 Infer\u00eancia Recorrente de Vendas - CVC Lojas\n",
                "\n",
                "## \ud83c\udfaf Objetivo\n",
                "Execu\u00e7\u00e3o peri\u00f3dica (Semanal/Mensal) para gerar novas previs\u00f5es de vendas.\n",
                "Este notebook n\u00e3o treina modelos. Ele carrega o modelo produtivo (`All-in-One`) e gera forecast baseando-se no hist\u00f3rico mais recente.\n",
                "\n",
                "## \u2699\ufe0f Fluxo de Execu\u00e7\u00e3o\n",
                "1.  **Context Loading:** Carrega os \u00faltimos 90 dias de vendas (Janela de Contexto) do Data Lake.\n",
                "2.  **Model Loading:** Baixa o modelo do Unity Catalog (`Usage: Production`).\n",
                "3.  **Inference:** O Wrapper `UnifiedForecaster` recebe o contexto, normaliza, prev\u00ea e desnormaliza.\n",
                "4.  **Persistence:** Salva os resultados na tabela `bip_vprevisao_lojas_futuro`.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- SETUP INICIAL ---\n",
                "%load_ext autoreload\n",
                "%autoreload 2\n",
                "\n",
                "import sys\n",
                "import os\n",
                "sys.path.append(os.getcwd())\n",
                "\n",
                "from src.validation.config import Config\n",
                "from src.validation.data import DataIngestion\n",
                "from datetime import timedelta, date\n",
                "import mlflow\n",
                "import pyspark.sql.functions as F\n",
                "import pandas as pd\n",
                "\n",
                "# Darts classes para Wrapper funcionar\n",
                "from darts import TimeSeries\n",
                "\n",
                "# Configs Spark\n",
                "spark.conf.set(\"spark.databricks.delta.optimizeWrite.enabled\", \"true\")\n",
                "spark.conf.set(\"spark.databricks.delta.autoCompact.enabled\", \"true\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- 1. DEFINI\u00c7\u00c3O DA JANELA DE CONTEXTO ---\n",
                "\n",
                "# Simulando \"Hoje\" (em prod usar date.today())\n",
                "today = date.today()\n",
                "\n",
                "# Janela de Lookback: precisamos de hist\u00f3rico suficiente para os Lags do modelo (ex: 60-90 dias)\n",
                "context_days = 90 \n",
                "start_context = today - timedelta(days=context_days)\n",
                "\n",
                "# Config Din\u00e2mica\n",
                "config = Config(spark)\n",
                "config.DATA_START = start_context.strftime(\"%Y-%m-%d\")\n",
                "config.INGESTION_END = today.strftime(\"%Y-%m-%d\")\n",
                "config.SCHEMA = \"cvc_pred\"\n",
                "\n",
                "print(f\"\ud83d\udcc5 Data de Refer\u00eancia (Hoje): {today}\")\n",
                "print(f\"\ud83d\udd0e Carregando contexto a partir de: {config.DATA_START}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- 2. CARREGAMENTO DO CONTEXTO (SPARK) ---\n",
                "# Reutilizamos a classe DataIngestion para garantir consist\u00eancia nas features\n",
                "ingestion = DataIngestion(spark, config)\n",
                "\n",
                "print(\"   \u23f3 Lendo dados hist\u00f3ricos recentes...\")\n",
                "df_context_spark = ingestion.create_training_set()\n",
                "\n",
                "# Filtro de Seguran\u00e7a e Sele\u00e7\u00e3o de Colunas\n",
                "df_context_spark = df_context_spark.filter(\n",
                "    F.col(\"DATA\").between(config.DATA_START, config.INGESTION_END)\n",
                ")\n",
                "\n",
                "# Traz para Pandas (Driver) - Volume pequeno pois \u00e9 s\u00f3 janela recente\n",
                "df_context_pd = df_context_spark.toPandas()\n",
                "df_context_pd['DATA'] = pd.to_datetime(df_context_pd['DATA'])\n",
                "\n",
                "# Injeta parametro 'n' para o wrapper saber o horizonte desejado\n",
                "FORECAST_HORIZON = 35\n",
                "df_context_pd['n'] = FORECAST_HORIZON\n",
                "\n",
                "print(f\"   \u2705 Contexto Carregado: {len(df_context_pd)} registros de vendas recentes.\")\n",
                "display(df_context_pd.head(5))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- 3. CARREGAMENTO DO MODELO ---\n",
                "\n",
                "model_name = f\"{config.CATALOG}.{config.SCHEMA}.cvc_lojas_forecast_production\"\n",
                "print(f\"\ud83d\udce5 Baixando modelo do Unity Catalog: {model_name}...\")\n",
                "\n",
                "# Em produ\u00e7\u00e3o real, voc\u00ea usaria um Alias como \"@Prod\" ou a vers\u00e3o specifica\n",
                "# modelo = mlflow.pyfunc.load_model(f\"models:/{model_name}@Prod\")\n",
                "# Aqui carregamos a \u00faltima vers\u00e3o logs\n",
                "loaded_model = mlflow.pyfunc.load_model(f\"models:/{model_name}/1\") \n",
                "print(\"   \u2705 Modelo Carregado!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- 4. PREVIS\u00c3O (SCORING) ---\n",
                "print(\"\ud83d\udd2e Gerando previs\u00f5es futuras...\")\n",
                "\n",
                "# O m\u00e9todo predict do nosso Wrapper j\u00e1 faz tudo:\n",
                "# 1. Recebe o DF brudo com hist\u00f3rico\n",
                "# 2. Reconstr\u00f3i a TimeSeries\n",
                "# 3. Aplica o Scaler salvo\n",
                "# 4. Prev\u00ea\n",
                "# 5. Inverte o Scaler\n",
                "forecast_df = loaded_model.predict(df_context_pd)\n",
                "\n",
                "# Enriquecimento final\n",
                "forecast_df['DATA_REFERENCIA_EXECUCAO'] = today\n",
                "\n",
                "print(f\"   \u2705 Previs\u00f5es geradas: {len(forecast_df)} registros.\")\n",
                "display(forecast_df.head())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- 5. PERSIST\u00caNCIA (WRITE BACK) ---\n",
                "output_table = f\"{config.CATALOG}.{config.SCHEMA}.bip_vprevisao_lojas_futuro\"\n",
                "\n",
                "print(f\"\ud83d\udcbe Salvando resultados em: {output_table}\")\n",
                "\n",
                "try:\n",
                "    (spark.createDataFrame(forecast_df)\n",
                "     .write\n",
                "     .format(\"delta\")\n",
                "     .mode(\"append\") # Append hist\u00f3rico de previs\u00f5es\n",
                "     .option(\"mergeSchema\", \"true\")\n",
                "     .saveAsTable(output_table)\n",
                "    )\n",
                "    \n",
                "    spark.sql(f\"OPTIMIZE {output_table}\")\n",
                "    print(\"   \u2728 Sucesso! Dados salvos e otimizados.\")\n",
                "except Exception as e:\n",
                "    print(f\"\u274c Erro ao salvar: {e}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}