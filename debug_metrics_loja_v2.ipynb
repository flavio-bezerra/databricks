{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7acc0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "# üîç Notebook de Debug: Valida√ß√£o de M√©tricas e Identifica√ß√£o de Lojas\n",
    "# Objetivo: Identificar por que o 'codigo_loja' n√£o est√° sendo salvo corretamente na tabela de m√©tricas.\n",
    "# Este notebook in-lineia e instrumenta as fun√ß√µes cr√≠ticas de `src/validation/data.py` e `src/validation/trainer.py`.\n",
    "\n",
    "# --- IMPORTS ---\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import pickle\n",
    "import os\n",
    "import hashlib\n",
    "import traceback\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mlflow\n",
    "from typing import List, Tuple, Optional, Any, Dict\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "import pyspark.sql.functions as F\n",
    "from mlflow.models import ModelSignature\n",
    "from mlflow.types.schema import Schema, ColSpec\n",
    "\n",
    "# Adiciona diret√≥rio raiz ao path para imports relativos funcionarem se necess√°rio\n",
    "sys.path.append(os.getcwd())\n",
    "\n",
    "from src.validation.config import Config\n",
    "from src.validation.data import DataIngestion \n",
    "from src.validation.pipeline import ProjectPipeline\n",
    "from src.validation.trainer import ModelTrainer\n",
    "from src.validation.models import DartsWrapper\n",
    "\n",
    "from darts import TimeSeries\n",
    "from darts.metrics import mape, mse, rmse, r2_score, smape\n",
    "from darts.models import LinearRegressionModel\n",
    "\n",
    "# --- CONFIGURA√á√ÉO ---\n",
    "if 'spark' not in locals():\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "config = Config(spark)\n",
    "print(f\"üîß Config Loaded: Version {config.VERSION}\")\n",
    "print(f\"üìÖ Data Start: {config.DATA_START}, Train End: {config.TRAIN_END_DATE}, Ingestion End: {config.INGESTION_END}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# --- 1. DATA INGESTION DEBUGGING (Exploded Function) ---\n",
    "# Esta fun√ß√£o substitui DataIngestion.build_darts_objects com prints de debug focados no indice de estaticas\n",
    "\n",
    "def build_darts_objects_debug(\n",
    "    df_spark_wide: DataFrame, \n",
    "    df_global_support: pd.DataFrame\n",
    ") -> Tuple[List[TimeSeries], List[TimeSeries]]:\n",
    "    \n",
    "    print(\"\\nüêõ [DEBUG] Iniciando build_darts_objects_debug...\")\n",
    "    \n",
    "    # 1. Spark para Pandas\n",
    "    print(\"‚öôÔ∏è Materializando dados do Spark para Pandas (Driver)...\")\n",
    "    df_wide = df_spark_wide.toPandas()\n",
    "    \n",
    "    # Dedup cleaning\n",
    "    df_wide = df_wide.loc[:, ~df_wide.columns.duplicated()]\n",
    "    \n",
    "    # Fix codigo_loja conversion issues\n",
    "    if \"codigo_loja\" in df_wide.columns:\n",
    "         col_obj = df_wide[\"codigo_loja\"]\n",
    "         if isinstance(col_obj, pd.DataFrame):\n",
    "              print(\"   ‚ö†Ô∏è CRITICAL: 'codigo_loja' is still a DataFrame (duplicate columns)!\")\n",
    "              df_wide = df_wide.loc[:, ~df_wide.columns.duplicated(keep='first')]\n",
    "\n",
    "    df_wide['data'] = pd.to_datetime(df_wide['data'])\n",
    "    \n",
    "    # Define Static Cols\n",
    "    possible_static = [\"cluster_loja\", \"sigla_uf\", \"tipo_loja\", \"modelo_loja\"]\n",
    "    static_cols = [c for c in possible_static if c in df_wide.columns]\n",
    "    \n",
    "    print(f\"   ‚ÑπÔ∏è Colunas Est√°ticas identificadas: {static_cols}\")\n",
    "\n",
    "    # 2. Criar Target Series\n",
    "    print(\"   Build: Criando Target Series (Vetorizado)...\")\n",
    "    try:\n",
    "        target_series_list = TimeSeries.from_group_dataframe(\n",
    "            df_wide,\n",
    "            group_cols=\"codigo_loja\",\n",
    "            time_col=\"data\",\n",
    "            value_cols=\"target_vendas\",\n",
    "            static_cols=static_cols,\n",
    "            freq='D',\n",
    "            fill_missing_dates=True,\n",
    "            fillna_value=0.0\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erro cr√≠tico no from_group_dataframe (Target): {e}\")\n",
    "        raise e\n",
    "\n",
    "    # --- DEBUG CR√çTICO: Verificar IDs nas Covari√°veis Est√°ticas ---\n",
    "    print(\"\\nüîé [DEBUG] Verificando IDs gerados em 'target_series_list' (RAW):\")\n",
    "    target_dict = {}\n",
    "    for i, ts in enumerate(target_series_list[:5]): # Mostra apenas os 5 primeiros\n",
    "        if ts.static_covariates is not None:\n",
    "            # Tenta pegar o ID do index\n",
    "            idx_name = ts.static_covariates.index.name\n",
    "            idx_val = ts.static_covariates.index[0]\n",
    "            print(f\"   üëâ Series[{i}] - Index Name: '{idx_name}' | Value: '{idx_val}' (Type: {type(idx_val)})\")\n",
    "        else:\n",
    "            print(f\"   ‚ùå Series[{i}] - Static Covariates is None!\")\n",
    "\n",
    "    # Processamento Normal\n",
    "    for ts in target_series_list:\n",
    "        if ts.static_covariates is not None and not ts.static_covariates.empty:\n",
    "            if ts.static_covariates.index.name == \"target_vendas\":\n",
    "                 ts.with_static_covariates(ts.static_covariates.rename_axis(\"codigo_loja\"))\n",
    "            \n",
    "            key_val = str(ts.static_covariates.index[0]).replace(\".0\", \"\")\n",
    "            target_dict[key_val] = ts\n",
    "    \n",
    "    valid_stores = list(target_dict.keys())\n",
    "    print(f\"   ‚úÖ Total Lojas V√°lidas (target_dict keys): {len(valid_stores)}\")\n",
    "    if len(valid_stores) > 0:\n",
    "        print(f\"   Exemplo de keys: {valid_stores[:5]}\")\n",
    "\n",
    "    # 3. Criar Covari√°veis Locais (Feriados)\n",
    "    # (Simplificado para o debug, focamos no target que carrega o ID principal)\n",
    "    # ... (Reusing original logic implicitly via filtered dataframe if needed, \n",
    "    # but for identifying the saving error, the target list is usually the source of truth for ordering)\n",
    "\n",
    "    # Retorna usando a logica original simplificada para focar no erro de ID\n",
    "    # Recriando lista ordenada\n",
    "    final_target_list = list(target_dict.values())\n",
    "    \n",
    "    # Mock de covari√°veis globais para n√£o quebrar pipeline\n",
    "    # Criando dummy covariates apenas para passar no pipeline\n",
    "    print(\"   Build: Gerando dummy covariates para teste...\")\n",
    "    full_covariates_list = []\n",
    "    for ts in final_target_list:\n",
    "        # Cria covari√°vel dummy zerada\n",
    "        cov = TimeSeries.from_times_and_values(\n",
    "            ts.time_index, \n",
    "            np.zeros((len(ts), 1)), \n",
    "            freq='D', \n",
    "            columns=['dummy_cov']\n",
    "        )\n",
    "        full_covariates_list.append(cov)\n",
    "        \n",
    "    return final_target_list, full_covariates_list\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# --- 2. MODEL TRAINER DEBUGGING (Exploded Function) ---\n",
    "\n",
    "def extract_id_debug(ts: TimeSeries, stage: str = \"UNKNOWN\") -> str:\n",
    "    \"\"\"Extrai ID com prints de debug\"\"\"\n",
    "    try:\n",
    "        if ts.static_covariates is not None:\n",
    "            if not ts.static_covariates.empty:\n",
    "                val = str(ts.static_covariates.index[0])\n",
    "                idx_name = ts.static_covariates.index.name\n",
    "                # print(f\"      [DEBUG ID {stage}] Found Val: '{val}', Index: '{idx_name}'\")\n",
    "                if val.endswith(\".0\"): val = val[:-2]\n",
    "                return val\n",
    "            else:\n",
    "                print(f\"      [DEBUG ID {stage}] Static Covariates is Empty!\")\n",
    "        else:\n",
    "            print(f\"      [DEBUG ID {stage}] Static Covariates is None!\")\n",
    "    except Exception as e:\n",
    "        print(f\"      [DEBUG ID {stage}] Exception: {e}\")\n",
    "    return \"UNKNOWN\"\n",
    "\n",
    "def train_evaluate_walkforward_debug(\n",
    "    config: Any,\n",
    "    train_series_static: List[TimeSeries], \n",
    "    full_series_scaled: List[TimeSeries], \n",
    "    val_series_original: List[TimeSeries], \n",
    "    target_pipeline: Any\n",
    ") -> None:\n",
    "    \n",
    "    print(\"\\nüêõ [DEBUG] Iniciando train_evaluate_walkforward_debug...\")\n",
    "    \n",
    "    # --- CHECK 1: Verificando IDs na entrada da fun√ß√£o (Scaled Series) ---\n",
    "    print(\"\\nüîé [DEBUG] CHECK 1: Inspecionando 'full_series_scaled' (Onde a l√≥gica original busca os IDs)...\")\n",
    "    debug_ids = []\n",
    "    for i, ts in enumerate(full_series_scaled[:5]):\n",
    "        extracted = extract_id_debug(ts, stage=\"SCALED_INPUT\")\n",
    "        debug_ids.append(extracted)\n",
    "        print(f\"   Series[{i}] (Scaled) -> Extracted ID: '{extracted}'\")\n",
    "        if ts.static_covariates is None:\n",
    "             print(\"   ‚ö†Ô∏è AVISO: static_covariates desapareceu ap√≥s scaling!\")\n",
    "    \n",
    "    ordered_store_ids = [extract_id_debug(ts, stage=\"SCALED_FULL\") for ts in full_series_scaled]\n",
    "    \n",
    "    # Valida quantos UNKNOWN temos\n",
    "    unknown_count = ordered_store_ids.count(\"UNKNOWN\")\n",
    "    print(f\"\\nüìä Total IDs extra√≠dos: {len(ordered_store_ids)}\")\n",
    "    print(f\"‚ö†Ô∏è Total 'UNKNOWN': {unknown_count}\")\n",
    "    print(f\"üìù Primeiros 10 IDs: {ordered_store_ids[:10]}\")\n",
    "    \n",
    "    if unknown_count == len(ordered_store_ids):\n",
    "        print(\"\\nüö®üö® ERRO CR√çTICO IDENTIFICADO: Todos os IDs s√£o UNKNOWN ap√≥s o scaling.\")\n",
    "        print(\"   CAUSA PROV√ÅVEL: O 'ProjectPipeline' ou seus Transformers est√£o removendo/resetando o √≠ndice das StaticCovariates.\")\n",
    "        print(\"   Recomenda√ß√£o: Verificar 'src/validation/pipeline.py' e 'StaticCovariatesTransformer'.\")\n",
    "        return # Para aqui pois n√£o adianta continuar\n",
    "\n",
    "    # Se tivermos IDs, simulamos a valida√ß√£o de 1 m√™s\n",
    "    print(\"\\nüîÑ Simulando loop de valida√ß√£o para verificar salvamento...\")\n",
    "    \n",
    "    # Mock predictions (copia do real para teste)\n",
    "    preds = full_series_scaled \n",
    "    \n",
    "    # Inverso transform\n",
    "    print(\"   Invertendo transforma√ß√£o (Inverse Transform)...\")\n",
    "    preds_inverse = target_pipeline.inverse_transform(preds, partial=True)\n",
    "    \n",
    "    # --- CHECK 2: Verificando IDs ap√≥s Inverse Transform ---\n",
    "    print(\"\\nüîé [DEBUG] CHECK 2: Inspecionando 'preds_inverse' (ap√≥s inverse transform)...\")\n",
    "    for i, ts in enumerate(preds_inverse[:3]):\n",
    "        extracted = extract_id_debug(ts, stage=\"INVERSE_PRED\")\n",
    "        print(f\"   PredsInverse[{i}] -> Extracted ID (pode ser perdido aqui, mas n√£o afeta ordena√ß√£o): '{extracted}'\")\n",
    "\n",
    "    # Simula _calc_metrics_and_format zip\n",
    "    print(\"\\nü§ù [DEBUG] Simulando ZIP para montar DataFrame de m√©tricas...\")\n",
    "    \n",
    "    res_dfs = []\n",
    "    # Zipar usando a lista ordered_store_ids original\n",
    "    for i, (ts_pred, ts_real_full, store_id) in enumerate(zip(preds_inverse, val_series_original, ordered_store_ids)):\n",
    "        if i >= 5: break # Apenas 5\n",
    "        \n",
    "        print(f\"   Itera√ß√£o {i}: Store ID from List = '{store_id}'\")\n",
    "        \n",
    "        try:\n",
    "            # Simula slice\n",
    "            ts_real_sliced = ts_real_full.slice_intersect(ts_pred)\n",
    "            \n",
    "            # Monta DF\n",
    "            df_row = pd.DataFrame({\n",
    "                'data': ts_pred.time_index,\n",
    "                'previsao': ts_pred.values().flatten(),\n",
    "                'real': ts_real_sliced.values().flatten(),\n",
    "                'codigo_loja': store_id, # << PONTO DE FALHA SE STORE_ID FOR UNKNOWN\n",
    "                'modelo': 'DEBUG_MODEL',\n",
    "                'metrica_mes': '2025-01'\n",
    "            })\n",
    "            res_dfs.append(df_row)\n",
    "            print(f\"     ‚úÖ DataFrame criado para '{store_id}'. Shape: {df_row.shape}\")\n",
    "            if store_id == \"UNKNOWN\":\n",
    "                print(\"     ‚ö†Ô∏è ALERTA: Salvando registro com codigo_loja='UNKNOWN'\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"     ‚ùå Erro na itera√ß√£o {i}: {e}\")\n",
    "\n",
    "    if res_dfs:\n",
    "        final_df = pd.concat(res_dfs)\n",
    "        print(\"\\nüìä Amostra do DataFrame Final:\")\n",
    "        print(final_df[['data', 'codigo_loja', 'previsao']].head())\n",
    "    else:\n",
    "        print(\"\\n‚ùå Nenhuma previs√£o gerada.\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# --- 3. EXECU√á√ÉO DO FLUXO DE DEBUG ---\n",
    "\n",
    "print(\"üöÄ INICIANDO PIPELINE DE DEBUG üöÄ\")\n",
    "\n",
    "# 1. Ingest√£o RAW (Usando original para pegar dados base)\n",
    "ingestion = DataIngestion(spark, config)\n",
    "# Pegamos apenas uma amostra ou dados reais filtrados\n",
    "print(\"üõí Carregando dados (pode demorar um pouco)...\")\n",
    "df_spark_wide = ingestion.create_training_set() # Spark DF\n",
    "\n",
    "# Filtra no Spark para ser r√°pido (apenas algumas lojas se poss√≠vel, ou usa tudo se n√£o for gigante)\n",
    "# df_spark_wide = df_spark_wide.limit(10000) # Opcional: limitar para teste r√°pido\n",
    "\n",
    "# 2. Build Objects (Logic Modified)\n",
    "df_support_global = ingestion.get_global_support()\n",
    "raw_series, raw_covs = build_darts_objects_debug(df_spark_wide, df_support_global)\n",
    "\n",
    "if not raw_series:\n",
    "    print(\"‚ùå Abortando: Nenhuma s√©rie retornada.\")\n",
    "else:\n",
    "    # 3. Pipeline Transform\n",
    "    print(\"\\nüõ†Ô∏è Executando ProjectPipeline (Fit/Transform)...\")\n",
    "    project_pipeline = ProjectPipeline()\n",
    "    \n",
    "    # Split simples para fit\n",
    "    train_cutoff_date = pd.Timestamp(config.TRAIN_END_DATE) - pd.Timedelta(days=1)\n",
    "    train_for_fit = [s.drop_after(train_cutoff_date) for s in raw_series]\n",
    "    cov_for_fit = [s for s in raw_covs] # Dummy covs\n",
    "    \n",
    "    project_pipeline.fit(train_for_fit, cov_for_fit)\n",
    "    \n",
    "    print(\"üîÑ Transformando s√©ries...\")\n",
    "    # AQUI OCORRE O ERRO POTENCIAL DE PERDA DE STATIC COVARIATES\n",
    "    series_scaled_full, cov_scaled_full = project_pipeline.transform(raw_series, raw_covs)\n",
    "    \n",
    "    # 4. Trainer Debug\n",
    "    train_evaluate_walkforward_debug(\n",
    "        config,\n",
    "        train_series_static=train_for_fit, # nao usado intensamente no debug\n",
    "        full_series_scaled=series_scaled_full, # << IMPORTANTE: Validar se index static persisitiu aqui\n",
    "        val_series_original=raw_series,\n",
    "        target_pipeline=project_pipeline\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
